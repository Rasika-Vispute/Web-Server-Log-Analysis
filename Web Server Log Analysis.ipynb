{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3551217",
   "metadata": {},
   "source": [
    "# Web Server Log Analysis  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eb0efa",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This assessment involves analyzing the Calgary HTTP dataset, which contains approximately one year's worth of HTTP requests to the University of Calgary's Computer Science web server. You'll work with real-world web server log data to extract meaningful insights and demonstrate your Python data analysis skills."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81debeba",
   "metadata": {},
   "source": [
    "## Part 1: Data Loading and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50c76827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import gzip\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7396603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file...\n",
      "Error downloading file: <urlopen error ftp error: error_reply('229 Extended Passive Mode Entered (|||25028|).')>\n"
     ]
    }
   ],
   "source": [
    "# Download .gz from FTP\n",
    "\n",
    "ftp_url = \"ftp://ita.ee.lbl.gov/traces/calgary_access_log.gz\"\n",
    "local_filename = \"calgary_access_log.gz\"\n",
    "\n",
    "try:\n",
    "    print(\"Downloading file...\")\n",
    "    urllib.request.urlretrieve(ftp_url, local_filename)\n",
    "    print(\"File downloaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"Error downloading file:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbc6303e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File read successfully with 726739 log entries.\n"
     ]
    }
   ],
   "source": [
    "# Read .gz file line by line\n",
    "\n",
    "log_lines = []\n",
    "\n",
    "try:\n",
    "    with gzip.open(local_filename, 'rt', encoding='latin1') as f:\n",
    "        for line in f:\n",
    "            log_lines.append(line.strip())\n",
    "    print(\"File read successfully with\", len(log_lines), \"log entries.\")\n",
    "except Exception as e:\n",
    "    print(\"Error reading the file:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b79750f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse each log line\n",
    "\n",
    "def parse_log_line(line):\n",
    "    pattern = (\n",
    "        r'(?P<host>\\S+) \\S+ \\S+ '\n",
    "        r'\\[(?P<timestamp>[^\\]]+)\\] '\n",
    "        r'\"(?P<request>[^\"]+)\" '\n",
    "        r'(?P<status>\\d{3}) (?P<size>\\S+)'\n",
    "    )\n",
    "    match = re.match(pattern, line)\n",
    "    if not match:\n",
    "        return None\n",
    "    \n",
    "    data = match.groupdict()\n",
    "\n",
    "    # Handle request (method, URL, protocol)\n",
    "    parts = data['request'].split()\n",
    "    if len(parts) == 3:\n",
    "        data['method'], data['url'], data['protocol'] = parts\n",
    "    else:\n",
    "        data['method'], data['url'], data['protocol'] = None, None, None\n",
    "\n",
    "    # Parse timestamp\n",
    "    try:\n",
    "        data['timestamp'] = datetime.strptime(data['timestamp'], \"%d/%b/%Y:%H:%M:%S %z\")\n",
    "    except:\n",
    "        data['timestamp'] = None\n",
    "\n",
    "    # Convert size\n",
    "    data['size'] = int(data['size']) if data['size'].isdigit() else None\n",
    "\n",
    "    # Extract file extension\n",
    "    if data['url']:\n",
    "        _, ext = os.path.splitext(data['url'])\n",
    "        data['file_ext'] = ext if ext else None\n",
    "    else:\n",
    "        data['file_ext'] = None\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ec5276a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed valid entries: 724836\n"
     ]
    }
   ],
   "source": [
    "# Apply parsing and clean malformed entries\n",
    "\n",
    "parsed_data = [parse_log_line(line) for line in log_lines]\n",
    "parsed_data = [entry for entry in parsed_data if entry is not None]\n",
    "\n",
    "print(\"Parsed valid entries:\", len(parsed_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75c2b6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>host</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>request</th>\n",
       "      <th>status</th>\n",
       "      <th>size</th>\n",
       "      <th>method</th>\n",
       "      <th>url</th>\n",
       "      <th>protocol</th>\n",
       "      <th>file_ext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>local</td>\n",
       "      <td>1994-10-24 19:41:41</td>\n",
       "      <td>GET index.html HTTP/1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>150.0</td>\n",
       "      <td>GET</td>\n",
       "      <td>index.html</td>\n",
       "      <td>HTTP/1.0</td>\n",
       "      <td>html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>local</td>\n",
       "      <td>1994-10-24 19:41:41</td>\n",
       "      <td>GET 1.gif HTTP/1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>1210.0</td>\n",
       "      <td>GET</td>\n",
       "      <td>1.gif</td>\n",
       "      <td>HTTP/1.0</td>\n",
       "      <td>gif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>local</td>\n",
       "      <td>1994-10-24 19:43:13</td>\n",
       "      <td>GET index.html HTTP/1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>3185.0</td>\n",
       "      <td>GET</td>\n",
       "      <td>index.html</td>\n",
       "      <td>HTTP/1.0</td>\n",
       "      <td>html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>local</td>\n",
       "      <td>1994-10-24 19:43:14</td>\n",
       "      <td>GET 2.gif HTTP/1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>2555.0</td>\n",
       "      <td>GET</td>\n",
       "      <td>2.gif</td>\n",
       "      <td>HTTP/1.0</td>\n",
       "      <td>gif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>local</td>\n",
       "      <td>1994-10-24 19:43:15</td>\n",
       "      <td>GET 3.gif HTTP/1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>36403.0</td>\n",
       "      <td>GET</td>\n",
       "      <td>3.gif</td>\n",
       "      <td>HTTP/1.0</td>\n",
       "      <td>gif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    host           timestamp                  request  status     size method  \\\n",
       "0  local 1994-10-24 19:41:41  GET index.html HTTP/1.0     200    150.0    GET   \n",
       "1  local 1994-10-24 19:41:41       GET 1.gif HTTP/1.0     200   1210.0    GET   \n",
       "2  local 1994-10-24 19:43:13  GET index.html HTTP/1.0     200   3185.0    GET   \n",
       "3  local 1994-10-24 19:43:14       GET 2.gif HTTP/1.0     200   2555.0    GET   \n",
       "4  local 1994-10-24 19:43:15       GET 3.gif HTTP/1.0     200  36403.0    GET   \n",
       "\n",
       "          url  protocol file_ext  \n",
       "0  index.html  HTTP/1.0     html  \n",
       "1       1.gif  HTTP/1.0      gif  \n",
       "2  index.html  HTTP/1.0     html  \n",
       "3       2.gif  HTTP/1.0      gif  \n",
       "4       3.gif  HTTP/1.0      gif  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load into DataFrame and clean types\n",
    "df = pd.DataFrame(parsed_data)\n",
    "\n",
    "# Drop rows with missing timestamps or URLs\n",
    "df_clean = df.dropna(subset=['timestamp', 'url'])\n",
    "\n",
    "# Convert to UTC and making timezone\n",
    "df_clean['timestamp'] = pd.to_datetime(df_clean['timestamp'], errors='coerce', utc=True)\n",
    "df_clean['timestamp'] = df_clean['timestamp'].dt.tz_localize(None)\n",
    "\n",
    "# Convert to the appropriate columns\n",
    "df_clean['status'] = pd.to_numeric(df_clean['status'], errors='coerce')\n",
    "df_clean['size'] = pd.to_numeric(df_clean['size'], errors='coerce')\n",
    "df_clean['file_ext'] = df_clean['file_ext'].astype('string').str.lstrip('.')\n",
    "df_clean['method'] = df_clean['method'].astype('category')\n",
    "df_clean['protocol'] = df_clean['protocol'].astype('category')\n",
    "\n",
    "# Preview clean data\n",
    "print(\"Cleaned DataFrame:\")\n",
    "display(df_clean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80da5c6e",
   "metadata": {},
   "source": [
    "## Part 2: Analysis Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afff13fe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Q1: Count of total log records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6264dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 1:\n",
      "722270\n"
     ]
    }
   ],
   "source": [
    "def total_log_records(df) -> int:\n",
    "    \"\"\"\n",
    "    Q1: Count of total log records.\n",
    "\n",
    "    Objective:\n",
    "        Determine the total number of HTTP log entries in the dataset.\n",
    "        Each line in the log file represents one HTTP request.\n",
    "\n",
    "    Returns:\n",
    "        int: Total number of log entries.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement logic to count log records\n",
    "\n",
    "    return len(df)  # Placeholder return\n",
    "\n",
    "\n",
    "answer1 = total_log_records(df_clean)\n",
    "print(\"Answer 1:\")\n",
    "print(answer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c5141e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Q2: Count of unique hosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcbccae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 2:\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "def unique_host_count(df) -> int:\n",
    "    \"\"\"\n",
    "    Q2: Count of unique hosts.\n",
    "\n",
    "    Objective:\n",
    "        Determine how many distinct hosts accessed the server.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of unique hosts.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement logic to count unique hosts\n",
    "    \n",
    "    return df['host'].nunique()  # Placeholder return\n",
    "\n",
    "\n",
    "answer2 = unique_host_count(df_clean)\n",
    "print(\"Answer 2:\")\n",
    "print(answer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c224d5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Q3: Date-wise unique filename counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac11c680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 3:\n",
      "{'01-Apr-1995': 407, '01-Aug-1995': 663, '01-Dec-1994': 244, '01-Feb-1995': 570, '01-Jan-1995': 82}\n"
     ]
    }
   ],
   "source": [
    "def datewise_unique_filename_counts() -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Q3: Date-wise unique filename counts.\n",
    "\n",
    "    Objective:\n",
    "        For each date, count the number of unique filenames that accessed the server.\n",
    "        The date should be in 'dd-MMM-yyyy' format (e.g., '01-Jul-1995').\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping each date to its count of unique filenames.\n",
    "              Example: {'01-Jul-1995': 123, '02-Jul-1995': 150}\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement logic for date-wise unique filename counts\n",
    "    \n",
    "    df = df_clean.copy()\n",
    "    df['date'] = df['timestamp'].dt.strftime('%d-%b-%Y')\n",
    "    return df.groupby('date')['url'].nunique().to_dict()   # Placeholder return\n",
    "\n",
    "\n",
    "answer3 = datewise_unique_filename_counts()\n",
    "print(\"Answer 3:\")\n",
    "# print(answer3)\n",
    "print(dict(list(answer3.items())[:5]))  # Show first 5 entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2da36a",
   "metadata": {},
   "source": [
    "### Q4: Number of 404 response codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0671865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 4:\n",
      "23376\n"
     ]
    }
   ],
   "source": [
    "def count_404_errors() -> int:\n",
    "    \"\"\"\n",
    "    Q4: Number of 404 response codes.\n",
    "\n",
    "    Objective:\n",
    "        Count how many times the HTTP 404 Not Found status appears in the logs.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of 404 errors.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement logic to count 404 errors\n",
    "    \n",
    "    df = df_clean.copy()\n",
    "    return (df['status'] == 404).sum()   # Placeholder return\n",
    "\n",
    "\n",
    "answer4 = count_404_errors()\n",
    "print(\"Answer 4:\")\n",
    "print(answer4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73928d2",
   "metadata": {},
   "source": [
    "### Q5: Top 15 filenames with 404 responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "358f0523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 5:\n",
      "[('index.html', 4694), ('4115.html', 902), ('1611.html', 649), ('5698.xbm', 585), ('710.txt', 408), ('2002.html', 258), ('2177.gif', 193), ('10695.ps', 161), ('6555.html', 153), ('487.gif', 152), ('151.html', 149), ('488.gif', 148), ('3414.gif', 148), ('40.html', 148), ('9678.gif', 142)]\n"
     ]
    }
   ],
   "source": [
    "def top_15_filenames_with_404() -> list[tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Q5: Top 15 filenames with 404 responses.\n",
    "\n",
    "    Objective:\n",
    "        Identify which requested URLs most frequently resulted in a 404 error.\n",
    "        Return the top 15 filenames sorted by frequency.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples (filename, count), sorted by count in descending order.\n",
    "              Example: [('index.html', 200), ...]\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement logic to find top 15 filenames with 404\n",
    "\n",
    "    df = df_clean.copy()\n",
    "    errors_404 = df[df['status'] == 404]\n",
    "    top_15 = errors_404['url'].value_counts().head(15)\n",
    "    return list(top_15.items())           # Placeholder return\n",
    "\n",
    "\n",
    "answer5 = top_15_filenames_with_404()\n",
    "print(\"Answer 5:\")\n",
    "print(answer5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6328c88a",
   "metadata": {},
   "source": [
    "### Q6: Top 15 file extension with 404 responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0aca8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 6:\n",
      "[('html', 12142), ('gif', 7202), ('xbm', 824), ('ps', 754), ('jpg', 520), ('txt', 496), ('GIF', 135), ('htm', 107), ('cgi', 77), ('com', 45), ('Z', 41), ('dvi', 40), ('ca', 36), ('hmtl', 30), ('util', 29)]\n"
     ]
    }
   ],
   "source": [
    "def top_15_ext_with_404() -> list[tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Q6: Top 15 file extensions with 404 responses.\n",
    "\n",
    "    Objective:\n",
    "        Find which file extensions generated the most 404 errors.\n",
    "        Return the top 15 sorted by number of 404s.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples (extension, count), sorted by count in descending order.\n",
    "              Example: [('html', 45), ...]\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement logic to find top 15 extensions with 404\n",
    "    \n",
    "    if 'file_ext' not in df_clean.columns:\n",
    "        df_clean['file_ext'] = df_clean['url'].str.extract(r'\\.([a-zA-Z0-9]+)$')[0].str.lower()\n",
    "    errors_404 = df_clean[df_clean['status'] == 404]\n",
    "    ext_counts = errors_404['file_ext'].value_counts().head(15)\n",
    "    return list(ext_counts.items())\n",
    "\n",
    "\n",
    "answer6 = top_15_ext_with_404()\n",
    "print(\"Answer 6:\")\n",
    "print(answer6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff52c8ba",
   "metadata": {},
   "source": [
    "### Q7: Total bandwidth transferred per day for the month of July 1995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45f52d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 7:\n",
      "{'01-Jul-1995': 16986893.0, '02-Jul-1995': 7892436.0, '03-Jul-1995': 11739190.0, '04-Jul-1995': 24976177.0, '05-Jul-1995': 22468066.0, '06-Jul-1995': 20419373.0, '07-Jul-1995': 9566244.0, '08-Jul-1995': 5475250.0, '09-Jul-1995': 4312672.0, '10-Jul-1995': 13195178.0, '11-Jul-1995': 22694805.0, '12-Jul-1995': 17861622.0, '13-Jul-1995': 15959344.0, '14-Jul-1995': 16143956.0, '15-Jul-1995': 17900110.0, '16-Jul-1995': 8086988.0, '17-Jul-1995': 18423405.0, '18-Jul-1995': 17947142.0, '19-Jul-1995': 16164044.0, '20-Jul-1995': 25504026.0, '21-Jul-1995': 25910651.0, '22-Jul-1995': 6224677.0, '23-Jul-1995': 10089651.0, '24-Jul-1995': 20554069.0, '25-Jul-1995': 23269918.0, '26-Jul-1995': 26191814.0, '27-Jul-1995': 22953465.0, '28-Jul-1995': 37450120.0, '29-Jul-1995': 16285535.0, '30-Jul-1995': 21147546.0, '31-Jul-1995': 29820800.0}\n"
     ]
    }
   ],
   "source": [
    "def total_bandwidth_per_day() -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Q7: Total bandwidth transferred per day for the month of July 1995.\n",
    "\n",
    "    Objective:\n",
    "        Sum the number of bytes transferred per day.\n",
    "        Skip entries where the byte field is missing or '-'.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping each date to total bytes transferred.\n",
    "              Example: {'01-Jul-1995': 123456789, ...}\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement logic to compute total bandwidth per day\n",
    "    \n",
    "    july_data = df_clean[df_clean['timestamp'].dt.strftime('%b-%Y') == 'Jul-1995']\n",
    "    july_data = july_data.dropna(subset=['size'])\n",
    "    july_data['date_str'] = july_data['timestamp'].dt.strftime('%d-%b-%Y')\n",
    "    bandwidth_by_day = july_data.groupby('date_str')['size'].sum().to_dict()\n",
    "    return bandwidth_by_day    # Placeholder return\n",
    "\n",
    "\n",
    "answer7 = total_bandwidth_per_day()\n",
    "print(\"Answer 7:\")\n",
    "print(answer7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc00908",
   "metadata": {},
   "source": [
    "### Q8: Hourly request distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a77f3e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 8:\n",
      "{0: 39503, 1: 32528, 2: 30632, 3: 28093, 4: 25919, 5: 22756, 6: 19705, 7: 16927, 8: 13811, 9: 11382, 10: 10527, 11: 10366, 12: 12261, 13: 15146, 14: 22027, 15: 30859, 16: 37909, 17: 46218, 18: 45658, 19: 49932, 20: 50954, 21: 52779, 22: 50316, 23: 46062}\n"
     ]
    }
   ],
   "source": [
    "def hourly_request_distribution() -> dict[int, int]:\n",
    "    \"\"\"\n",
    "    Q8: Hourly request distribution.\n",
    "\n",
    "    Objective:\n",
    "        Count the number of requests made during each hour (00 to 23).\n",
    "        Useful for understanding traffic peaks.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping hour (int) to request count.\n",
    "              Example: {0: 120, 1: 90, ..., 23: 80}\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement logic for hourly distribution\n",
    "\n",
    "    hourly_data = df_clean.dropna(subset=['timestamp'])\n",
    "    hourly_data['hour'] = hourly_data['timestamp'].dt.hour\n",
    "    hour_counts = hourly_data['hour'].value_counts().sort_index().to_dict()\n",
    "    return hour_counts     # Placeholder return\n",
    "\n",
    "\n",
    "answer8 = hourly_request_distribution()\n",
    "print(\"Answer 8:\")\n",
    "print(answer8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363b7083",
   "metadata": {},
   "source": [
    "### Q9: Top 10 most requested filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91168ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 9:\n",
      "[('search', 15), ('451', 14), ('~mildred', 10), ('547', 5), ('A><BR>', 4), ('~hyatt', 4), ('OA_presentation', 4), ('groupkit', 3), ('Policy', 3), ('GeneralAgenda', 3)]\n"
     ]
    }
   ],
   "source": [
    "def top_10_most_requested_filenames() -> list[tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Q9: Top 10 most requested filenames.\n",
    "\n",
    "    Objective:\n",
    "        Identify the most commonly requested URLs (irrespective of status code).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples (filename, count), sorted by count in descending order.\n",
    "                Example: [('index.html', 500), ...]\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement logic to find top 10 most requested filenames\n",
    "    \n",
    "    df_clean['filename'] = df_clean['url'].str.extract(r'/([^/?#]+)(?:[?#]|$)', expand=False)\n",
    "    valid_files = df_clean.dropna(subset=['filename'])\n",
    "    top_10_files = valid_files['filename'].value_counts().head(10)\n",
    "    return list(top_10_files.items()) # Placeholder return \n",
    "\n",
    "\n",
    "answer9 = top_10_most_requested_filenames()\n",
    "print(\"Answer 9:\")\n",
    "print(answer9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedb4778",
   "metadata": {},
   "source": [
    "### Q10: HTTP response code distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd4453ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 10:\n",
      "{200: 566192, 302: 30260, 304: 97560, 400: 13, 401: 46, 403: 4738, 404: 23376, 500: 42, 501: 43}\n"
     ]
    }
   ],
   "source": [
    "def response_code_distribution() -> dict[int, int]:\n",
    "    \"\"\"\n",
    "    Q10: HTTP response code distribution.\n",
    "\n",
    "    Objective:\n",
    "        Count how often each HTTP status code appears in the logs.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping HTTP status codes (as int) to their frequency.\n",
    "              Example: {200: 150000, 404: 3000}\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement logic for response code counts\n",
    "    \n",
    "    valid_status = df_clean.dropna(subset=['status'])\n",
    "    status_counts = valid_status['status'].value_counts().sort_index()\n",
    "    return status_counts.to_dict()    # Placeholder return\n",
    "\n",
    "\n",
    "answer10 = response_code_distribution()\n",
    "print(\"Answer 10:\")\n",
    "print(answer10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7063ebc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
